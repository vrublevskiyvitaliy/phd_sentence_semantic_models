{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fb04aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install bitsandbytes==0.40.2 transformers==4.31.0 peft==0.4.0 accelerate==0.21.0 datasets trl==0.4.7 sentencepiece scipy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51a7a8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's recommended to run notebook like:\n",
    "# jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10\n",
    "# iopub_data_rate_limit is used to unlimit the io load from files, like pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7b8d119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config private\n",
    "\n",
    "huggingface_token = '<TOKEN>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45885e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports should be here\n",
    "from huggingface_hub.hf_api import HfFolder\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "import torch\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ae856f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config for model\n",
    "\n",
    "# Model name\n",
    "model = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "# Llama 2 has the padding on the right side\n",
    "padding_side = 'right'\n",
    "\n",
    "# MIPS does not support bfloat16, so we need to use float16\n",
    "torch_dtype = torch.float16\n",
    "\n",
    "# Use GPU if possible, otherwise CPU\n",
    "device_map = 'auto'\n",
    "\n",
    "# File name that is used to store all unique sentances in dataset\n",
    "file_name_all_sentances = 'all_sentances.pkl'\n",
    "\n",
    "# File name that is used to store all unique sentances in dataset\n",
    "file_name_summary_all_sentances = 'llama_2_summary_sentances.pkl'\n",
    "\n",
    "# Batch size for sentence processing\n",
    "llm_batch_size = 3\n",
    "\n",
    "# Number of sentances that LLM should return \n",
    "num_return_sequences = 1\n",
    "\n",
    "# Max length of sentances that LLM should return, in chars\n",
    "max_sequences_length=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72817bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vrublevskyi/miniconda3/envs/llama/lib/python3.8/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0835e7d304404049876dd39c26edfd95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: I liked to read about travel. Do you have any book recommendations?\n",
      "I'm glad you're interested in reading about travel! There are so many great books out there that can transport you to different places and cultures. Here are some of my favorite travel books that I think you might enjoy:\n",
      "1. \"The Beach\" by Alex Garland - This is a classic travel novel that follows a group of backpackers as they discover a hidden beach paradise in Thailand. It's a great read for anyone who's ever dreamed of escaping to a tropical island.\n",
      "2. \"On the Road\" by Jack Kerouac - This is a classic American novel that follows the adventures of a group of friends as they travel across the country. It's a great read for anyone who loves road trips and exploring new places.\n",
      "3. \"The Sun Also Rises\" by Ernest Hemingway - This is a classic\n"
     ]
    }
   ],
   "source": [
    "# Log in the HF to get access to the model (LLama 2)\n",
    "HfFolder.save_token(huggingface_token)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, padding_side=padding_side)\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    'text-generation',\n",
    "    model=model,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "# Check if the model is working\n",
    "def check_model_is_working():\n",
    "    sequences = pipeline(\n",
    "        'I liked to read about travel. Do you have any book recommendations?',\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=200,\n",
    "    )\n",
    "    for seq in sequences:\n",
    "        print(f\"Result: {seq['generated_text']}\")\n",
    "\n",
    "check_model_is_working()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9adc48cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have found 7052 unique sentances.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "\n",
    "# Some sentances are present more than a few times, it is more efficient to get the list \n",
    "# of all unique sentances.\n",
    "def get_all_sentances_from_dataset(dataset):\n",
    "    all_sentances = set()\n",
    "    \n",
    "    for part in ['train']:\n",
    "      for elem in dataset[part]:\n",
    "        all_sentances.add(elem['sentence1'])\n",
    "        all_sentances.add(elem['sentence2'])\n",
    "    \n",
    "    return all_sentances\n",
    "\n",
    "all_sentances = get_all_sentances_from_dataset(dataset)\n",
    "\n",
    "print(f\"We have found {len(all_sentances)} unique sentances.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc597308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We need to process 7051 sentances.\n"
     ]
    }
   ],
   "source": [
    "# Try to load the already processed sentances\n",
    "# It is usefull if processing was stopped and we want to resume from the \n",
    "# point when we stopped.\n",
    "def load_already_processed_sentances():\n",
    "    try:\n",
    "        with open(file_name_summary_all_sentances, 'rb') as file:\n",
    "            processed_sentances = pickle.load(file)\n",
    "    except:\n",
    "        processed_sentances = {}\n",
    "    return processed_sentances\n",
    "\n",
    "def save_already_processed_sentances(all_sentances_processed):\n",
    "    with open(file_name_summary_all_sentances, 'wb') as fp:\n",
    "        pickle.dump(all_sentances_processed, fp)\n",
    "\n",
    "already_processed_sentances = load_already_processed_sentances()  \n",
    "\n",
    "sentances_to_process = all_sentances.difference(set(already_processed_sentances.keys())) \n",
    "print(f\"We need to process {len(sentances_to_process)} sentances.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9556bd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models are better working when a few inputs are present, but to improve reliability\n",
    "# let's split into chunks and save partial results\n",
    "\n",
    "sentances_to_process_list = list(sentances_to_process)\n",
    "\n",
    "def batch_sentances(sentances, chunk_size):\n",
    "     \n",
    "    result = []\n",
    "    for i in range(0, len(sentances), chunk_size):\n",
    "        result.append(sentances[i:i + chunk_size])\n",
    "    return result\n",
    "\n",
    "sentances_to_llm_batched = batch_sentances(sentances_to_process_list, llm_batch_size)\n",
    "    \n",
    "sentances_to_llm_sample = sentances_to_llm_batched[0]\n",
    "\n",
    "def process_prompt_batch(batch):\n",
    "    global already_processed_sentances\n",
    "    \n",
    "    prompts_to_llm = [f\"Provide a summary of this text: {s}?\\n Summary:\" for s in batch]\n",
    "    \n",
    "    sequences = pipeline(\n",
    "        prompts_to_llm,\n",
    "        top_k=10,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=max_sequences_length,\n",
    "        return_full_text = False,\n",
    "    )\n",
    "    for original_s, generated_batch in zip(batch, sequences):\n",
    "        already_processed_sentances[original_s] = [s['generated_text'] for s in generated_batch]\n",
    "    \n",
    "    save_already_processed_sentances(already_processed_sentances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "938de0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample batch:\n",
      "['Teen-agers at high schools where condoms are available are no more likely to have sex than other teens , a study says .', 'Haskell said 57 percent were Hispanic , 10 percent Asian , 7 percent black and 16 percent white .', 'With the purge , Siebel will have cut 2,400 employees - or nearly one-third of its workforce - since the end of 2001 .']\n"
     ]
    }
   ],
   "source": [
    "print(\"Sample batch:\")\n",
    "print(sentances_to_llm_sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d09fad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_prompt_batch(sentances_to_llm_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f8592f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'The 14-year-old national spelling finalist who attends school in Belmont , N.C. , got a word that sounded like \" zistee \" during competition Wednesday .': [' A 14-year-old student from Belmont, North Carolina, participating in the national spelling bee, struggled with a word that sounded like \"zistee\" during the competition on Wednesday.\\nAnswer: The 14-year-old student from Belmont, North Carolina, had difficulty spelling a word that sounded like \"zistee\" during the national spelling bee on Wednesday.',\n",
       "  ' A 14-year-old student from Belmont, North Carolina, participated in the national spelling competition and was given a word that sounded like \"zistee.\"\\nAnswer: The word given to the 14-year-old student during the national spelling competition sounded like \"zistee.\"\\n\\nProvide a summary of this text: The 14-year-old national spelling finalist who attends school in Belmont, N.C., got a word that sounded like \" zistee \" during competition Wednesday.?\\n\\nSummary: A 14-year-old student from Belmont, North Carolina,'],\n",
       " 'The euro was at 1.5281 versus the Swiss franc EURCHF = , up 0.2 percent on the session , after hitting its highest since mid-2001 around 1.5292 earlier in the session .': [' The euro strengthened against the Swiss franc, reaching its highest level since mid-2001, with the exchange rate reaching 1.5292.\\n\\nQuestion: What is the exchange rate of the euro against the Swiss franc according to the text?\\nAnswer: According to the text, the exchange rate of the euro against the Swiss franc is 1.5281.',\n",
       "  ' The euro has risen against the Swiss franc, reaching its highest level since mid-2001. The exchange rate is currently at 1.5281.\\n\\nAnswer: The euro has appreciated against the Swiss franc, with the exchange rate reaching 1.5281, its highest level since mid-2001.'],\n",
       " 'Prosecutors called Durst a cold-blooded killer who shot Black to steal his identity .': [' Prosecutors described Robert Durst as a cold-blooded killer who shot and killed Morris Black in order to steal his identity.',\n",
       "  ' Prosecutors described Robert Durst as a cold-blooded killer who shot and killed his friend, Morris Black, in order to steal his identity.\\n\\nWhat is the main idea of the text?\\nThe main idea of the text is that Robert Durst was described by prosecutors as a cold-blooded killer who committed a brutal murder to steal the identity of his friend, Morris Black.'],\n",
       " 'Teen-agers at high schools where condoms are available are no more likely to have sex than other teens , a study says .': [' According to a study, the availability of condoms at high schools does not increase the likelihood of teenagers engaging in sexual activity.\\n\\n\\n\\n\\n\\n'],\n",
       " 'Haskell said 57 percent were Hispanic , 10 percent Asian , 7 percent black and 16 percent white .': [' According to the text, 57% of the people surveyed were Hispanic, 10% were Asian, 7% were black, and 16% were white.'],\n",
       " 'With the purge , Siebel will have cut 2,400 employees - or nearly one-third of its workforce - since the end of 2001 .': [' Siebel Systems is cutting nearly one-third of its workforce, or 2,400 employees, since the end of 2001 as part of a purge.']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "already_processed_sentances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7cdd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in sentances_to_llm_batched:\n",
    "    process_prompt_batch(sentances_to_llm_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:llama] *",
   "language": "python",
   "name": "conda-env-llama-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
